\section{Discussion}
\label{sec:discussion}

In this section, we discuss \ac{name} in a broader context, examining its
reliance on the integrity of its data sources and its performance in
the face of increasing \ac{https} adoption. We conclude with a discussion of
future work.

\paragraph{Data Source Integrity}

\ac{name} relies on the integrity of the data sources (in our current prototype,
Censys and \ac{ct}). Therefore, \iac{ca} who issues
and publicizes an unauthorized certificate for a domain that does not deploy
\ac{https} can cause the domain to be included in the signal set and thus render
the domain inaccessible. This fragility effectively allows an adversary to
``poison the well'' that \ac{name} relies on, and execute a denial-of-service
attack with just a single unauthorized certificate.

To recover from such an attack, the
affected domain could request that the \ac{ca} revoke the certificate if the
\ac{ca} issued the certificate in error. If the \ac{ca} deliberately misissued
the certificate, the browser vendor could treat the certificate as revoked
(though such action is unlikely unless the domain is popular). The affected
domain could also upgrade to \ac{https}, obtaining enough certificates to
override the misissued certificate. 
%In the event that the domain wishes to
%remain without \ac{https}, we can allow domains to set a policy flag that
%signals that the domain will communicate over HTTP. The domain, however, must
%still send this policy and proof during connection establishment.

Enhancements to certificate logs to mitigate situations such as this are ongoing. Some
\ac{ct} logs, for example, only accept certificates from certain \acp{ca}, and
nearly all of them perform some level of certificate validation, including
checking whether a certificate is anchored in a known root certificate. We
expect an attack that involves poisoning data sources in this way to be possible
but rare and noticeable, and an attacker seeking to carry out \iac{mitm} in this way has a
higher hurdle for the attack to succeed than in today's Web \ac{pki}.

\paragraph{The Role of Log Aggregator}
\bryan{TODO}

\paragraph{Increasing \ac{https} Adoption}

In the past several years, Let's Encrypt and similar efforts to increase
the use of \ac{https} in the Internet has caused a marked increase in the number
of domain names served over \ac{tls}, as shown in
\autoref{sec:evaluation:https}. Due to the structure of DNS, it is impossible to
know how many domain names there are overall, and thus difficult to determine a
ceiling for the number of names known to serve content over \ac{https}.
Therefore, it is possible that the signaling set may grow to a size that is
untenable for increasingly many clients.
However, this may also be offset by the upward trend in available bandwidth
(e.g., the upcoming move to 5G).
We could also mitigate the memory and disk usage for clients by splitting the 
signaling set into several \ac{dafsa} representations based 
on a client's region or self-selected categories of interest.
We also considered splitting the DAFSA based on popularity,
but conversations with Mozilla indicate this approach is not palatable
as it is seen as perpetuating inequality on the Web;
hence any solution should ideally treat websites large and small 
in an egalitarian fashion~\cite{privatecomm}.

\paragraph{Future Work}

One avenue of future exploration that is currently ongoing is that of
\emph{transition compaction}, which aims to minimize our \ac{dafsa}
representation by reducing the representation size of each transition. A
transition in the representation consists of a symbol and destination state. We
have found that in the current signaling set representation, the symbols and
destination states had a long-tailed distribution. Thus by using variable-length
encodings, we can reduce the overall size of each transition. One
straightforward way to do so is via Huffman encoding, but the long-tailed
distribution results in a Huffman dictionary whose size is significant enough to
neutralize any space savings that would result. We are currently exploring the
use of selecting a subset of symbols to Huffman encode. For destination states,
we can assign an ordering of states in the \ac{dafsa} such that the difference
to each destination state in the ordering is small, and we can then use integer encoding to
represent the location of these states. We are currently exploring the
possibility of vertex separators in graphs to address this problem.

%We also plan to examine the use of multiple \acp{dafsa}, collecting data on
%domain names including relative popularity, region, and category. We can then
%construct multiple \ac{dafsa} according to our existing approach, and evaluate
%disk space, memory usage, and connection latency (particularly if we dynamically
%load \acp{dafsa}). We note that some of the necessary data is available from services
%as Alexa Web Information Services and DNS resolvers.
