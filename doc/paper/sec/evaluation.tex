\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate the performance and overhead of \ac{name}. In
particular, we examine the performance and overhead of our implementations of
the clien, signaling authority, and the log aggregator, respectively.
\steve{TODO: update}

\subsection{Growth of HTTPS Deployment}

\begin{compactitem}
\item Motivation: empirical basis for how storage requirements are expected to
  scale over time
\item Sources: Censys, CT Logs
\item Results
  \begin{compactitem}
  \item Discrepancy between number of domains in Censys and CT (almost a factor
    of 10)
    \begin{compactitem}
    \item Currently: CT Google logs' valid name set grows from 113M to 168M in
      the period of 1/1/17 to 12/4/17 (roughly 164k names per day)
    \item Could indicate that Censys has low coverage due to errors of poor
      server reliability, or that the vast majority of certificates are never
      seen on the Web in practice
    \end{compactitem}
  \item Seems to be a fairly regular growth rate over time, with periods of high
    growth corresponding to events in the community (e.g., CA removal or CT
    requirement)
  \item \steve{TODO: how has Let's Encrypt deployment affected this?}
  \item \steve{TODO: expiration rate?}
  \item \steve{TODO: what kinds of domains are newly deploying HTTPS?}
  \end{compactitem}
\end{compactitem}

\subsection{Client}

We prototyped the client software as a Mozilla Firefox plugin.

\begin{compactitem}
\item Update sizes
\item Memory usage
\item Disk storage size
\item Connection latency
\end{compactitem}

\steve{TODO: how much space efficiency do we lose if we separate out domains?}

\steve{TODO: also compare with set of domains covered by HTTPS Everywhere for
comparison}

\subsection{Signaling Authority}

\begin{compactitem}
\item Changes to the signaling set (what domains change and overall growth)
\item Time to create/update set
\end{compactitem}

\subsection{Log Aggregator}

\begin{compactitem}
\item Size of aggregated log data over time (shows storage overhead and growth)
\item Mapping size vs. number of certificates
\end{compactitem}

%\subsection{Policy Mechanism}

%\draft{We implemented a log that tracks policies. Our code is in \steve{TODO}.
%We measured its performance and storage requirements given realistic rates of
%new certificates.}

%\subsection{Signaling Mechanism}

%We implemented our signal set construction in C++.

%\draft{We compare our approach to simple compression of the list of deploying
%domains, and to succinct encodings of a prefix tree (trie). We measure the
%compressed size, memory footprint, and added connection latency.}

%\steve{TODO: Add projections on how the metrics will change if there are $n$
%deploying domains.}

%\textbf{Failures of a Probabilistic Approach.} While at first it may seem that
%data structures implementing probabilistic membership queries (i.e., Bloom
%filters~\cite{bloom1970space} and their variants) could provide much of the
%benefits of a signaling set with only a small cost, this approach has several
%critical shortcomings. First, Bloom filters allow false positives but no false
%negatives. Therefore, while clients are protected from \ac{mitm} attacks that
%could result from false negatives, clients may be blocked from accessing sites
%that do not deploy \ac{https} because they are false positives in the Bloom
%filter. Second, on average, a Bloom filter with false positive probability
%\bloomfpp requires approximately $-\ln \bloomfpp/(\ln 2)^2$ bits per inserted
%element in the optimal case. Thus for a false positive rate of $p = 0.01$ (which
%would still cause 10,000 sites out of 1M sites deploying \ac{https} to be
%blocked), we would require 9.59 bits per site on average. Thus for our full data
%set, a Bloom filter would require \steve{145 MB}.
